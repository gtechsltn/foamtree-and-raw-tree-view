modelDataAvailable({"MultilingualClustering.languageCounts":{"":11},"processing-time-algorithm":1443,"documents":[{"snippet":"Letter of Recommendation – Subhasis Dutta\nIt gives me immense pleasure to recommend Subhasis Dutta for a Master’s of Science program in Computer Science in your esteemed University. My interaction with him started when he had just completed his under graduation and was looking for a way to carry forward his entrepreneurial venture dealing with Open Source and Robotics. Though we were not able to start this specific venture, we have since then collaborated and successfully delivered in many other open source and commercial projects dealing with Big Data.\nSubhasis is a self-motivated, determined and hard working engineer. As co-founder of Svapas Innovations – a multi-disciplinary incubator, I mentor many companies like Serendio, Arogita, and Infilytics Inc. Out of his own interest to learn and solve challenging problems, Subhasis contributed to some core IP that these companies were developing in the areas of fraud detection in Insurance claims and log analysis from multiple sources in a scalable infrastructure. He has also worked directly with me in building prototypes in the areas of Medical Diagnostics and Robotics. \nAs his mentor, I have always encouraged him to pursue higher studies to support his curiosity and passion for problem solving. A Master’s degree will definitely help him in this regard. I am convinced that he will put forth all his effort into any endeavor he undertakes and endorse his candidacy for graduate studies with great confidence.\n","title":"Letter_of_Recommendation-Subhasis.txt","id":"0"},{"snippet":" \nIndex\n \n\ttwitterapi (socialnuggets)\n \n\tLogin details\n\teReader Login\n \n\tTV NewLogin\n \n\tTelevision Login\n \n\tBankingLogin\n \n\tSmartphones Login\n \n\tTablets Login\n \n\tNotebooks-Netbooks Login\n \n\tTIECON Login\n \n\tiPhone4S Login\n \n\tGaming Console (general) Login\n \n\tAmazon KindleFire Login\n \n\tSprint Login\n \n\tCrutchfield Followers Login\n \n\tPoliticsLogin\n \n\tOriental CuisineLogin\n \n\tConsole Gaming (NextWealth specific) Login\n \n\tSony TV Login\n \n\tSony Camera Login\n \n\tElectrolux VacuumLogin\n \n\tElectrolux French Door Refrigerator Login\n \n\tWhirlpool Top Load Washers Login\n \n\tWhirlpoolMicrowave Login\n  \n\tSentimentids\n \n\tLanguageid\n \n\tWordType\n \n\tCreating NewWorkspace\n \n\tAdding Products to ExistingWorkspace\n \n\tAdd Words to ExistingWorkspace\n \n\tEdit Existing Features\n \n\tEdit Existing Products\n \n\tUpload Postscollected\n \n\tXML Rules\n \n\tUrls for Downloads(from Voom)\n  \nTo do Daily:\n \n\tOpenhttp://www.socialnuggets.net/staging/?q=twitterapi\n \n\tSelect “Automobiles”\n \n\tSelect a model / Company name\n \n\tClick on “right arrow” button (next to the search field)\n \n\tCheck if any results are displayed (should be viewable below the search field)\n  \nDO NOT CLOSE THIS PAGE\n \nIN CASE OF NO RESULTS\n \n\tOpenhttp://78.46.77.101:8080/\n \n\tEnter some number, say 300 in the 1stfield (“Validity, min time (default 60sec, 0 for full table)”)\n \n\tEnter the model / Company name (random) in the “Search String”\n \n\tClick on “Send”\n \n\tCheck if any results are displayed (should appear in the text box at the top of the screen)\n \n\tIf yes, then check again for a greater number, say 15000 (repeat steps 3, 4 & 5)\n\tIf results are displayed, then click on “Stop”\n \n\tGo back to the openhttp://www.socialnuggets.net/staging/?q=twitterapi, and click on the “pause” button to stop the process\n  \n\tIf no, then inform Arun / Harsha immediately\n  \nLOGIN\n \ne-Reader\n \nWorkspace id=85eb6b66-f2c9-4ee3-854a-498180804947\n \nhttp://voom.serendio.com/ereadersgpvoomsrv/login?user_name=test&password=098f6bcd4621d373cade4e832627b4f6&login=\n \nPortal:http://customers.serendio.com/demo/ereaders/\n \nTVNew\n \nWorkspace id=114305b4-2927-46f7-91d9-839df1070f24\n \nhttp://voom.serendio.com/tvnewsrv/login?user_name=test&password=098f6bcd4621d373cade4e832627b4f6&login=\n  \nPortal:http://customers.serendio.com/demo/televisions/\n  \nTelevision\n \nWorkspace id=fae2699a-c4ab-47f4-b1c2-edb1aa9f28ca\n \nhttp://voom.serendio.com/televisionsgpvoomsrv/login?user_name=test&password=098f6bcd4621d373cade4e832627b4f6&login=\n \nPortal: http://customers.serendio.com/demo/televisions/\n \nBanking:\n \nWorkspace id =abf955e6-b041-487f-8479-3cbc01a81ac6\n \nhttp://voom.serendio.com/gpvoomsrv/login?user_name=test&password=098f6bcd4621d373cade4e832627b4f6&login=\n \nPortal:http://customers.serendio.com/demo/banking/\n \nSmartphones:\n \nWorkspace id =b69fda41-0b1e-4059-9aa9-81b906b55fed\n \nhttp://voom.serendio.com/smartphonesgpvoomsrv/login?user_name=test&password=098f6bcd4621d373cade4e832627b4f6&login=\n \nPortal:http://customers.serendio.com/demo/smartphones/#\n  \nTablets:\n \nWorkspace id =72b63081-26a6-491a-89e7-d94786674262\n \nhttp://voom.serendio.com/gpvoomsrv2/login?user_name=test&password=098f6bcd4621d373cade4e832627b4f6&login=\n \nPortal:http://customers.serendio.com/demo/tablets/#\n \nNotebooks-Netbooks\n \nWorkspace id= 3a05c01a-8407-4352-b147-576705fd2f13\n \nhttp://voom.serendio.com/notebooksgpvoomsrv/login?user_name=test&password=098f6bcd4621d373cade4e832627b4f6&login=\n \nPortal:http://customers.serendio.com/demo/notebooks-netbooks/#\n \nTIECON\n \nWorkspace id= d630a876-cbe7-4a95-9c91-890131019722\n \nhttp://voom.serendio.com/tieconsrv/login?user_name=test&password=098f6bcd4621d373cade4e832627b4f6&login=\n \nPortal:http://customers.serendio.com/demo/tiecon/\n  \niPhone 4S\nWorkspace id= 5801eda9-053d-458a-8c3f-2f13741890e3\n \nhttp://voom.serendio.com/iphonesrv/login?user_name=test&password=098f6bcd4621d373cade4e832627b4f6&login=\n \nPortal:http://customers.serendio.com/demo/iphone4s/\n \nGaming Console (general)\n \nWorkspace id= f398ee85-7578-414d-a44f-6e5665f6bf88\n \nSignon:\nhttp://voom.serendio.com/gaminggpvoomsrv/login?user_name=test&password=098f6bcd4621d373cade4e832627b4f6&login=\n \nPortal:http://customers.serendio.com/demo/gamingconsole\n \nAmazon Kindle Fire\n \nWorkspace id =cc6d5cad-a98d-4d3c-bcfe-fa75a0538730\n \nSignon:\nhttp://voom.serendio.com/kindlefiresrv/login?user_name=test&password=098f6bcd4621d373cade4e832627b4f6&login=\n \nPortal:http://customers.serendio.com/demo/kindlefire/\n \nSprint\n \nWorkspace id=112926a5-ed9e-4be7-b148-4df5082e4149\n \nSignon:\nvoom.serendio.com/sprintsrv/login?user_name=test&password=098f6bcd4621d373cade4e832627b4f6&login=\n \nPortal:http://customers.serendio.com/demo/sprintapps/\n  \nCrutchfield Followers\n \nWorkspace id=4b63f35b-c1f9-40cc-871f-18868e57e71f\n \nSignon:http://voom.serendio.com/crutchfieldsrv/login?user_name=test&password=098f6bcd4621d373cade4e832627b4f6&login=\n  \nPolitics\n \nWorkspace id=2c801d45-b9ee-4fd8-9f19-b4af3c54d9c3\n \nSignon:http://voom.serendio.com/politicsgpvoomsrv/login?user_name=test&password=098f6bcd4621d373cade4e832627b4f6&login=\n \nPortal:http://customers.serendio.com/demo/politics/\n \nOriental Cuisine\n \nWorkspace id=0425e0d2-88ca-4dbb-9e1a-65d3b8fe0bfa\n \nSignon:http://voom.serendio.com/orientalcuisinegpvoomsrv/login?user_name=test&password=098f6bcd4621d373cade4e832627b4f6&login=\n \nPortal:http://customers.serendio.com/demo/orientialcuisine/\n  \nFor NextWealth\n \nConsole Gaming (NextWealth specific):\n \nWorkspace id =4a110d82-f1ca-458a-a6fd-15d502bf87f4\n \nSignon: \nvoom.serendio.com/sonygpvoomsrv/login?user_name=test&password=098f6bcd4621d373cade4e832627b4f6&login=\n \nPortal:http://customers.serendio.com/demo/nwconsolegaming\n  \nSony TV:\n \nWorkspace id = 97a4b8c8-d59e-48d9-b889-839190d05d50\n \nSignon:\nvoom.serendio.com/sonygpvoomsrv/login?user_name=test&password=098f6bcd4621d373cade4e832627b4f6&login=\n \nPortal:http://customers.serendio.com/demo/nwtelevisions\n \nSony Camera\n \nWorkspace id = ade4f14b-af35-4e32-bb25-becfaf59a505\n \nSignon:\nvoom.serendio.com/sonygpvoomsrv/login?user_name=test&password=098f6bcd4621d373cade4e832627b4f6&login=\n \nPortal:http://customers.serendio.com/demo/nwcamera\n \nElectrolux Vacuum\n \nWorkspace id= 7e6ea4bc-b0ec-4f83-8a7b-be1cc4d753ef\n \nSignon:\nhttp://voom.serendio.com/electroluxgpvoomsrv/login?user_name=test&password=098f6bcd4621d373cade4e832627b4f6&login=\n \nPortal:http://customers.serendio.com/demo/nwvacuums\n  \nElectrolux French Door Refrigerator\n \nWorkspace id=a24f6e5e-2b5e-4a58-8e13-30c724535ae6\n \nSignon:\nhttp://voom.serendio.com/electroluxgpvoomsrv/login?user_name=test&password=098f6bcd4621d373cade4e832627b4f6&login=\n \nPortal:http://customers.serendio.com/demo/nwfrenchdoor\n \nWhirlpool Top Load Washers\n \nWorkspace id=ac88a7ea-f3cc-4247-adfb-0f3040a2e840\n \nSignon:\nhttp://voom.serendio.com/whirlpoolgpvoomsrv/login?user_name=test&password=098f6bcd4621d373cade4e832627b4f6&login=\n \nPortal:http://customers.serendio.com/demo/nwtoploadwashers\n \nWhirlpool Microwave\n \nWorkspace id=7bdff8fc-3ce4-4934-bdae-34d7cce85356\n \nSignon:\nhttp://voom.serendio.com/whirlpoolgpvoomsrv/login?user_name=test&password=098f6bcd4621d373cade4e832627b4f6&login=\n \nPortal:http://customers.serendio.com/demo/nwmicrowave\n  \nSentiment ids\n \n\t88642e5e-3782-4510-93d6-5efc2ec3404c –Positive\n \n\t45b9f99e-cfe5-4c90-beb5-34c5eb93dbe7 -Negative\n \n\tcf73b38c-7755-469f-bfd3-52897b0d8455 -Neutral\n \n\t00a5c37a-2517-461e-ac5f-99535fec059b –Mixed\n  \nLanguage id\n \nEnglish: 8002def8-53f8-430e-9433-23ab2b50ae25\n  \nWord Type\n \n\tblind_negation_word\n \n\tdimensionword\n \n\tignore_word\n \n\tnegation_word\n \n\tsentimentword\n \n\tsplit_word\n  \nCreate new domain / add new product\n \nNEW WORKSPACE:\nXYZ creates a new workspace, and gives the login.\n \nLogin to the given workspace, then\n \nCreate new category id:\n\thttp://voom.serendio.com/tvnewsrv/category/add?name=(Nameof the product/domain – for eg., “TV New”). This will generate the category id for the given product / domain. Copy this category id onto the product & feature xml which is ready for upload.\n  \nAdd new feature list (completely) –this will create a new set of features – only to be used when creating new domain, or when overwriting an existing domain\n\thttp://voom.serendio.com/tvnewsrv/feature/batchAddForm: Upload the ready feature xml file. If successful, open new tab, check the feature list (by using list feature command).\n  \nAdd new product (completely) –this will create a new set of products – only to be used when creating new domain, or when overwriting an existing domain\n\thttp://voom.serendio.com/tvnewsrv/product/batchAddForm: Upload the ready product xml file. If successful, open new tab, check the product (by using list product command).\n  \nAdd new sentiment word list (initially & later)\n \n\tPopulate the category id column in the sentiment words file in relevant column\n \n\thttp://voom.serendio.com/<VOOMSRV>/word/batchAddForm: Upload the ready csv file.\n \n\tIf successful, check the uploaded words list by voom.serendio.com/<VOOMSRV>/word/listCsv?category_id=<CATEGORY_ID>&word_type=<WORD_TYPE>&language_id=<LANGUAGE_ID>&sentiment_id=<SENTIMENT_ID>\n  \nEXISTING WORKSPACE:\n \n\tAdd each product individually to the respective parent id using the “add product” url (given\n  \nbelow).\n \n\tCopy & paste the product ids which are generated onto the xml file.\n  \n\tOnce all products are added,http://voom.serendio.com/(server\n  \n\tname)/product/batchUrlAddForm: Upload the ready product xml file. If successful, open\n\tnew tab, check the product (by using list product command).\n \n\tAfter upload, check with Saravana if he has\n  \n\tdisabled the sentiment extraction\n \n\tfixed the broken link between two tables\n  \nEdit (existing)\n \nFeature:\n \nhttp://voom.serendio.com/testvoomsrv/feature/add?name=(resp feature to be added)&parent_id=(resp id under which feature should appear)&category_id=(give category id)\n \nif at the parent level (directly under the All Features id),\nhttp://voom.serendio.com/smartphonesgpvoomsrv/product/add?name=(Name of product)&parent_id=(All features id under which product is to appear)&category_id=(resp category id)\n \nfor any updates:\n \nnew name:\nhttp://server/feature/update?id=(id of the feature to be updated)&new_name=(new name if feature name is to be changed)\n \nnew parent id:\nhttp://server/feature/update?id=(id of the feature to be updated)&new_parent_id=(id of the new parent feature under which this is to be moved)\n \nFeature Rename:\nhttp://voom.serendio.com/smartphonesgpvoomsrv/feature/update?id=(idof the feature to be renamed)&new_name=(give the new name)&category_id=(give category id)\n \nFeature Synonym:\n \nhttp://voom.serendio.com/testvoomsrv/feature/addSynonym?id=(feature id under which synonym is to be added)&synonym=(resp synonym to be added)\n \nProduct:\nhttp://voom.serendio.com/(server_name)/product/add?name=(Name of product)&parent_id=(parent id under which product is to appear)&category_id=(resp category id)\n \nif at the parent level(directly under the category id),\nhttp://voom.serendio.com/smartphonesgpvoomsrv/product/add?name=(Name of product)&category_id=(resp category id)\n \nname change:\nhttp://voom.serendio.com/smartphonesgpvoomsrv/product/update?id=(product id of the product torename)&new_name=(new name of the product)\n \ndelete:\nhttp://server/product/delete?id=(product id of the product to be deleted)\n \nProduct Synonym:\nhttp://voom.serendio.com/(servername)/product/addSynonym?id=(product id for which synonym is to be added)&synonym=(synonym)\n \nProduct xml:\nhttp://voom.serendio.com/(servername)/product/batchUrlAddForm - add the xml file containingthe urls to be uploaded\n \nWords:\n \nCollect words in the required format & ensure no extra space is present in any column. Upload using thehttp://voom.serendio.com/<gpvoomsrv_name>/word/batchAddForm\n  \nCollected posts:\nProcess I (old)\n  \n\tSign on to the portal.\n \n\thttp://voom.serendio.com/testvoomsrv/data/curateform\n \n\tUpload the file\n \n\tConfirm the upload with the tech team\n  \nProcess II (current)\n \n\tAfter post collection, create a file mapping xml (like a normal product xml)\n \n\tIn the <urlid=””>, the url should look like this: file:///home/deployer/manualcrawl/Sep_29/Kindle.csv\n \n\tOnce the file mapping xml is ready, upload it using batchUrlAddForm\n \n\tDownload the product xml to check if this url has been added\n \n\tAfter upload, send the csv file(s)& xml to Saravana\n  \nPost Collection\n \n\tThe file should be a csv file (“name.csv”) – the name should not contain any space; preferable is a single word\n  \n\tThe file name should be the name of the product\n  \n\tThe file should have the following columns: “author”, “posted_date”, “source”, “source_type”, “entity”, “title”, “uri” & “data”\n\tauthor: Name of the author (prefix ' if name starts with a number)\n \n\tposted_date: format: dd-mmm-yyyy\n \n\tsource: the site (for eg., amazon.com)\n \n\tsource_type: “review” OR “forum”\n \n\tentity: “post”\n \n\ttitle: given title of the post, or the first 50 characters of the actual post\n \n\turi: the permalink of the post (if unavailable, the url that is the nearest match)\n \n\tdata: The post content\n  \nXMLRules:\n \n\tproduct / feature xml, when first uploaded, shouldcontain the category id\n  \n\tproduct synonyms should be added individually onto the server directly\n  \n\tproduct xml should contain the product ids (obtained from the server)\n  \n\tany further product xml for the existing domain should NOT contain category id – it should start with “urllist”, and there will be no hierarchy structure\n  \nDownloads\n \nGuide Document:\nhttp://voom.serendio.com/docs/\n \nFor everything listed below, login to the respective server is needed\n \nProduct xml:\nhttp://voom.serendio.com/(server name)/product/list?category_id=(ID) \n \nFeature List:\nhttp://voom.serendio.com/(server name)/feature/list?category_id=(ID) \n \nSite list (list of sites crawled for specific product):\nvoom.serendio.com/testvoomsrv/export/export?category_id=(ID)&list=true\n \nExport (old):\nhttp://voom.serendio.com/testvoomsrv/export/export?category_id=(ID)&start_date=2010-01-01 00:00&end_date=2010-03-30 23:00&curated=false\n \nExport (site specific) - (old):\nhttp://voom.serendio.com/testvoomsrv/export/export?category_id=(givecategory id)&sites=(sites, with comma to seperate)&start_date=2010-11-01 00:00&end_date=2010-12-14 23:00&curated=false\n \nSentiment Word list:\n \nhttp://voom.serendio.com/gpvoomsrv2/word/listCsv?category_id=(give category id)&word_type=sentimentword&language_id=(give language id)&sentiment_id=(give sentiment id) \n  \n\t\tlogin\n \n\ttype http://voom.serendio.com:19555/gpadidasoldsrv/data/getCategory; click enter\n \n\tselect category / product\n \n\tupload csv file, click submit\n  \nSQL Query for getFollowers:\n \n\tensure the getFollowers script has completed its run at the DB level\n  \n\tLogin to phpMyAdmin athttp://5.9.115.14/phpMyAdmin/index.php?db=twitter_stats&table=follower_stats&target=tbl_export.php&token=e3102735eae0ffd5a0d38da8a4963115#PMAURL:db=twitter_stats&table=follower_stats&target=tbl_sql.php&token=241ff1a3c3676b7b2dc34b7c4c6dc7da\n  \nUser name : root ; password: $erendi0$\n \n\tSelect “twitter_stats”, then “follower_stats”, then “SQL” tab\n  \n\tClear the existing query, and enter this query:\n  \nSELECT name, following, screen_name, statuses_count, followers_count, friends_count, (\nfollowers_count - friends_count\n) * statuses_count AS score\nFROM follower_stats\nWHERE following = '<twitter handle>'\nORDER BY score DESC\nLIMIT 500\n \n\tClick “GO”\n  \n\tScroll to bottom of page, click on “Export”, select file type – the list is exported – ENJOY!!\n  \n","title":"Voom_New_-_Copy_-_Copy_-_Copy.txt","id":"1"},{"snippet":"","title":"TESTING_REPORT.txt","id":"2"},{"snippet":"The Art of Hiring… Starts with Training?\n \nHiring good quality talent for Big Data Science is a challenge even in , and in , it is even more pronounced. As a Big Data startup in , we realized unless we took control of the situation, our raison d'etre would be in jeopardy. So we have decided to launch our Training program – a program with emphasis on hands-on programming, taught by our Developers who will focus on imparting skills we care about. Before deciding to do this, we explored numerous options including working with Training institutes. But their focus was always on giving out certificates, focusing on a very simplistic syallabi, cookie cutter projects, and above all the quality of teaching very specious. \nWhat will our training offer?\nThe Big Data Science technology stack is fast evolving with new and better techniques coming in every day and we at Serendio create and exploit such techniques. Knowing when to use what technologies is becoming an important trait of Big Data engineers. And our development team’s collective experience in this space is going to help in imparting this skill to our potential hires.\nWe will be using our soon to be launched Training as a way to filter and identify quality talent for our own hiring needs. Post the training, we will be conducting programming tests and make job offers to those who meet our criteria. We are flipping the model of Hiring first, Training next pursued by major IT companies like Infosys, Cognizant etc. For a startup to indulge in Training the way we plan to do is unheard of…but we are not afraid to take the Hiring bull by its horns. \n \nin India seems to be quite a challenge for us in the Big Data Science field. We want to be a start-up that is in more control of the situation. We came through this breakthrough idea where instead of Hiring and Training, we figured how would it be if we Trained and Hired? Being in this business since a long time, we know that we are not a company who wants to have massive hires as the likes of Infosys, Cognizant and TCS. Our space is very niche, different and focused.\nProblems with hiring in Big Data\nProgramming for big data science applications is a trickier affair, and IT departments that are staffed with people who understand the old database technologies like SQL are ill-equipped to tackle the world of Big Data (Hadoop, MapReduce).\n Major Big Data technologies being open source, do-it-yourself are not the most intuitive and easy-to-use technology. We want to be a firm who is ahead of the curve by leveraging this hiring issue and creating opportunities for all stakeholders.\nWe are a company working in Big Data Science since four years headquartered in Silicon Valley and with offices in Chennai and Bangalore. We work in Data Analytics intensive sectors as Healthcare, Education and Retail. As we know, the Big Data industry software’s and technologies keep changing every twelve months due to the ever changing nature of the industry. For example correct predictions are getting tougher to make because technology in addition to analyzing also allows users to make snap buying decisions and switch brand or company loyalty. The data that we are dealing with has also changed from the onset of the internet. We've been steadily moving from text-based communications to richer data that include images, videos, and interactive maps as well as metadata such as geo-location information and time and date stamps. \nYou are the professional going to be in great demand, since the industry is facing a dearth of us, the people who can work in the challenging dynamic of changing the traditional approach of data mining to the innovative of Big Data Science. \nWhat will this training offer?\nTraining will focus on mastery of skills and not certification. It will bridge the gap between the skills that the workforce has and what the industry expects.\n A dynamic curriculum is planned as technology obsolescence is a characteristic of Big Data. \nThe curriculum and training will be decided and taught by Big Data Science Developers and not trainers, because we believe only a person with a good exposure to real world projects and case studies can impart maximum learning.  \n \nIt will aid in our hiring issues and is not meant for companies such as Infosys and Cognizant that work in massive hire and fire sprees. We are very selfish from the outset, we want to hire the best guys out there in the workforce and what better way than Train and Hire. Since the motive of the program is to master the skills, we will offer full support to the students to come and re-attend the same course they have enrolled for how many ever times they want during the course of a year. The motivation is very clear, we want to train and hire you either full-time, part-time or offer an internship with Serendio. Are You Game?\n  \n","title":"training-to-hiring.txt","id":"3"},{"snippet":"Twitter User Conversation Analyzer \n  \n(Second Review Report)\n  \nDate:24-feb-2015\n \nRegistration Number\n:  2010239011\n \nName of the Candidate\n: Kovalan.R\n \nName of the Company\n: Serendio Softwares Pvt.Ltd\nLiterature Survey (Phase II)\n        The objective of this work is to automatically identifying the influence people from twitter and extract the structure of arguments from those people generated content, to display this structure and to help a industry or organization to market there product by using their influence on Social Media. There is a system (TEM Engine) on current approaches for extracting arguments from user generated content to find sentiment analysis and Kloud score, Influence score. This work differs from other areas of argument analysis as it aims to extract arguments from within a stream of conversations from user generated content sources such as Tweets, favorites and re-tweets  on twitter and other media as yet undefined; therefore specific conversations must be identified and appropriate sections of text extracted.  \n  \nIn addition, there is an intention to combine the extraction of the argument structure with tools available for visualization of these arguments. The proposed steps in this process are inspired by fields such as influence mining. The plan for the extraction of the data is adapted from the steps they define. The steps in the process, and therefore the problems that this work aims to tackle, are: \n \n1. Identify network of user on different sub-network of user based on their followers.\n \n2. Identify spans of text that illustrate the discussion. \n \n3. Classify into a structure so as to define the relationships between spans of text.\n \n4. Present this information to customers for their marketing purpose.\n \nTwitter users can choose to post a tweet in a number of different manners: \n \n• Public tweets – appear in the public Twitter stream  \n \n• Reply tweets – also public but directed at another Twitter user through the use of the @Kovalan (i.e., @KovalanR).\n \n• Retweets – forwarded messages, allowing the user to redirect a tweet from another user to his/her tweet stream (similar to a quote of someone else’s message) All posts will instantly appear on the user’s homepage, as well as to anyone who follows that person, with the newest messages appearing at the top of the list.\nOverall Architecture / System Set / Block Diagrams\n  \nData Flow Diagrams / ER Diagrams\nNumber of Modules – Proposed & Number of Modules – Completed\nModules – Proposed:\n \n            Complete UI design and development with crawled data.\n \n            Complete Report Development\n \n            Staging\n \n            Testing.\n \n                        Deployment. \n \n                        Customer Support.\n          Modules – Completed:\n            Complete UI design and development with crawled data.\n \n            Complete Report Development\n \n            Staging\nConfirmation of tools for Analysis / Design\nRequirement Analysis:\nMicrosoft Word for Documentation\n \nMicrosoft PowerPoint for Presentation\nSmart Draw and Microsoft Tools for Models and Diagrams\nSmartDraw is a visual processor used to create flowcharts, organization charts, mind maps, project charts, and other visuals. SmartDraw is built exclusively for Windows operating systems and works with Since version 7, it uses Microsoft’s Fluent  User Interface in conjunction with automated panels specific to each type of diagram.\nDesign/ Development :\nTwitter Bootstrap Framework\n \nMysql \n \nNutch Web Crawler \nNutch is an effort to build an open source web search engine based on Lucene and Java for the search and index component. Nutch is coded entirely in the Java programming language, but data is written in language-independent formats. It has a highly modular architecture, allowing developers to create plug-ins for media-type parsing, data retrieval, querying and clustering.\nSublime Editor\n \nHadoop \n \nEclipse\n \nHBase \n \nDJango Web Server\nDjango is a high-level Python Web framework that encourages rapid development and clean, pragmatic design.\nApache Solr (Search Engine)\nSolr is the popular, blazing fast open source enterprise search platform from the Apache Lucene project. Its major features include powerful full-text search, hit highlighting, faceted search, near real-time indexing, dynamic clustering, database integration, rich document (e.g., Word, PDF) handling, and geospatial search. Solr is highly reliable, scalable and fault tolerant, providing distributed indexing, replication and load-balanced querying, automated failover and recovery, centralized configuration and more. Solr powers the search and navigation features of many of the world's largest internet sites.\nFurther work to be done\nTesting.\n \nDeployment. \n \nCustomer Support\n    Apache Solr\r\n  \n            Browser (UI)\r\n  \n","title":"TUCA.txt","id":"4"},{"snippet":"  \nCampaign Analytics Platform\n Statement of Work & Proposal\n \nClient: OPT America\n  \nwww.serendio.com \n \n9/19/14\n  \nScope & Objectives\nOPT America is desirous of building a Campaign Analytics Platform (CAP) to monitor, analyze, and enhance the effectiveness of the digital campaigns (incentives, coupons, and other offers) that OPT designs and executes on behalf of Brands and/or Retailers. Shown below are the various stages of a typical OPT digital campaign.\nDATA FLOW IN A OPT CAMPAIGN\n \nStep 1:   Campaign Design – OPT along with the Brand/Retailer define the Campaign by identifying the Product (SKU), and deciding on the discount/incentive, store locations, target demographics, validity period and other campaign specific attributes.\n \nStep 2:  Campaign Execution -- OPT along with its partners (Mobile coupon partner, Digital Agency) tag and distribute the coupons/incentives to consumers through one or more of mobile, social, website, email channels. Campaign metadata is pushed into CAP so as to associate the campaign performance with specific campaigns.\n \nStep 3: Consumer Engagement – Consumers print/save the coupons and redeem them in one of the valid store locations. This who redeemed what coupon when & where, who downloaded a coupon when but never redeemed, store inventory data all become part of the 1st party data. Note: The who does not identify the individual and in our context will represent an aggregate group, since retail stores would be reluctant to share individual consumer information.  \n \nStep 4: 1st party data generated from step 3 along with 3rd party data (DMPs, weather bureau, neighbourhood demographics, social media etc.) are pushed into CAP for analyzing campaign effectiveness.\n \nStep 5: CAP provides a flexible way to analyze the aggregated campaign data including a dashboard comparing the actual campaign performance to preset campaign KPIs, and an automated alert/signalling module to dynamically adjust the campaign for better performance. Over time, CAP will also support a historical campaign data warehouse to simulate, hypothesize, and predict future campaign performance.\n  \nImplementation Overview\nShown below are the key building blocks of CAP:\n  \nHighlights include:\nBatch and/or real-time data ingestion (ETL) into CAP based on source and data type. \nData cleansing, extraction, and transformation logic will be written as Hadoop Map-Reduce jobs, thus exploiting parallel processing constructs for performance and scale\nData replication to ensure business continuity and a robust data security and access control will be supported in the platform\nHBase (a NoSQL data warehouse) and Spark (in-memory computing) will be key technologies to address the scalability and performance needs. The schema-less nature of NoSQL allows a wide variety of data sources to be ingested and analyzed on the fly.\nDescriptive and Predictive analytics will be built using Tableau or Splunk combined with R and various Machine Learning packages (MLLib) that run on Spark.  Details of the Predictive models including choice of algorithms and outcomes will be highlighted in a separate document once the use cases have been provided by OPT. \nThe self-serve analytics portal will be built around Tableau/Splunk and will allow users to create custom charts/graphs, manage passwords, setup alerts and email notifications, export data out of the system etc. \nTasks & Timelines\nWe propose a 3-step implementation strategy for CAP:\na Demo platform to prove technology/architecture appropriateness with simulated campaigns\na POC to prove business value with real manufacturer/retail data\na MVP (Minimum Viable Product) built by enhancing the POC to showcase data scalability, and increased campaign effectiveness through predictive analytics and near real-time campaign performance feedback.\n  \nDemo\nBig Data Platform \nInferential Analytics\nSimulated 1st party data\n \nPOC\n \nData security\nOne end-to-end campaign\n Real 1st party data\nMVP\nScalability & Performance\nPredictive   Modelling\n1st and 3rd party data\nNoSQL Data warehouse\nMultiple Campaigns\n  \nDemo Platform\nBusiness Goal:  Run a simulated campaign (synthetic data) for 1 SKU, 1 Retailer, multiple locations\nTechnical Goal:  Hadoop based data and analytics infrastructure, support for simulated 1st party data only, integration with an open source BI tool \nTimeline: 3 months\nResources: 1 Architect/Project Manager, 1 FT Data Scientist, 1 FT Data Engineer\n \nPOC\nBusiness Goal:  Run an optimized, real campaign for 1 SKU, 1 Retailer, multiple locations\nTechnical Goal:  Build the Demo platform out with integration with Tableau/ Splunk, integration with Campaign Execution Engine for pulling campaign metadata and pushing alerts/actions.\nTimeline: 2 months\nResources: 1 Architect/Project Manager, 1 FT Data Scientist, 2 FT Data Engineers\n \nMVP\nBusiness Goal:  Run concurrent campaigns for multiple Brands, Retailers, Products, and multiple locations\nTechnical Goal:  Enhance the scalability, performance, and latency characteristics of the POC version of the platform, support for other data sources including 3rd party data, Weather, Demographics, Predictive models and Campaign performance simulation.\nTimeline: 3 months\nResources: 1 Architect/Project Manager, 2 FT Data Scientists, 2 FT Data Engineers\n \nManaged Services \nManage the CAP hosting infrastructure; provide ongoing customization, enhancement, maintenance and support of CAP\nResources: 1 Engineering Manager, 1 FT Support Engineer, 1 FT Sys Admin, 2 FT Data Scientists, 2 FT Data Engineers\n \nNote: Resources for all phases will be a combination of US and off shore (India) based. A detailed project plan with key milestones will be provided at the start of the project.\n  \nDeliverables \nThe following will be delivered over the Demo, POC and MVP phases:\nDesign and code for CAP that supports import of data from various sources, data scrubbing/mapping/correction logic, and store scrubbed data in HBase\nDesign and code the self-serve Analytics platform integrated with Tableau (or Splunk), R, and machine learning libraries \nOut-of-the-box predictive models\nDocumentation (Architecture, Schema & Code), Source Code, Test cases\nAppropriate Performance & Scalability metrics plus Hardware sizing\nCode Review, Knowledge sharing, Training to OPT team, if needed\n \nTechnology Stack\nHortonWorks or Cloudera Hadoop 2.x\nProgramming language – Java\nHBase will be the NoSQL campaign data warehouse \nMySQL where NoSQL technology performs poorly or is an overkill.\nData access APIs, where needed, will be RESTful.\nSelf-serve Analytics portal will be developed using Tableau or Splunk\nAll technologies used will be Open source with Apache license except for Tableau/Splunk, which are commercial offerings.\n  \nPricing \nDemo Platform\n        Manpower = $60k (1 Architect/Project Manager, 1 FT Data Scientist, 1 FT Data Engineer)\n       Infrastructure = $5k\n       Software Licensing = $0\n       Total = $65k\nPOC\n        Manpower = $50k (1 Architect/Project Manager, 1 FT Data Scientist, 2 FT Data Engineers)\n       Infrastructure = $5k\n       Software Licensing (Splunk/Tableau) = $25k\n       Total = $80k\n \nMVP\nManpower = $125k (1 Architect/Project Manager, 2 FT Data Scientist, 2 FT Data Engineers)\nInfrastructure = $25k\nSoftware Licensing (Splunk/Tableau) = $50k\n3rd party data licensing = $25k\nTotal = $225k\n \nManaged Services (cost/year)\nManpower = $450k (1 Engineering Manager, 1 FT Support Engineer, 1 FT Sys Admin, 2 FT Data Scientists, 2 FT Data Engineers)\nInfrastructure = $100k (will be dependent on the number of campaigns)\nSoftware Licensing (Splunk/Tableau) = $100k\n3rd party data licensing = $100k (ballpark)\nTotal = $750k \nAny Travel will be billed at actuals and will be undertaken only after prior consent from OPT.\nThe infrastructure/data center provider for development and hosting the POC and MVP, and for any performance benchmarking needs will be decided jointly by OPT & Serendio. It is assumed all deployments will be on a single tenant model so as to adhere to data security and privacy requirements.\n \nReferences\n1. OPT Requirements – phone and email conversations with Michael Dudynskay <Mike@opt-america.com>\n  \nIN WITNESS WHEREOF, the parties hereto have executed this Agreement by their duly authorized officers or representatives.\nSERENDIO INC.\tCOMPANY\t\t\t\nSignature:\t\tSignature:\t\t\nPrinted Name:\t\tPrinted Name:\t\t\nTitle:\t\tTitle:\t\n  \nCampaign Analytics Platform\n                        HADOOP BIG DATA STACK\n            CAMPAIGN PERFORMANCE ANALYTICS\n                   (KPIsl; Alerts, Reach & Conversion Prediction)\n1st & 3rd Party Data \n Inventory  & Shipping data\nCampaign Metadata\nHistorical Campaigns\nE T L\nAGGREGATED, CLEANSED DATA\n                             PERFORMANCE            \t     VISUALIZATION\nPERFORMANCE FEEDBACK\n  \n","title":"CAP_proposal.txt","id":"5"},{"snippet":"M/s. A.R. Builders\nTo. \t\t\t\t\t\t\t\t\t\t\tDate : \nMr. A. Thirunavukarasu,\t\t\t\t\t\t\t\tChennai\nS/o. Mr. T.R. Arumugam,\nFlat No. 5, Arun Flats,\nNo.18, Ellaiamman Kovil Street,\nWest Mambalam,\nChennai – 600033.\n \nDear Sir,\nSub- Housing flats at Madipakkam – allotment of covered car parking – letter of confirmation - Reg.\nRef  - Flat No. S2 – Block “A”\nAs per the Construction Agreement date 29-7-2011, we are pleased to allot the covered car park bearing No. A-S2 in the South East Corner in “A” Block basement (slit) measuring about \nNorth to South on the Eastern side \t: 13 feet\nNorth to South on the Western side \t: 13 feet\nEast to West on the Northern side \t: 7 Feet\nEast to West on the Southern side \t: 7 Feet\nIn all measuring about 91 sqft., \nThe above space should be unutilized only for parking of respective vehicle and no construction or alteration should be made in the above car park in mere future.\nFor A. R. Builders\n(Mrs. Alseema)\nProprietrix\n","title":"Letter.txt","id":"6"},{"snippet":"DAVID PAYAB, ESQ. (SBN 187887)\nLEONARD KIRSCHEN, ESQ. (SBN 228334)\nPAYAB & ASSOCIATES\n5850 Canoga Avenue, Suite 400\nWoodland Hills, Califomia 91367\nTelephone: (818) 888-4546\nTelecopier: (818)888-4547\nAttomeys for Plaintiffs\nMARIAM, INC., a Califomia Corporation; KAM-RON LIMITED LIABILITY COMPANY, a\nCalifornia Limited Liability Company; 8950 MEMORY PARK I, L.P., a Califomia Limited\nPartnership; KAMRAN MEHDIZADEH, an individual; and MEHRAN MEHDIZADEH, an\nindividual\nSUPERIOR COURT OF THE STATE OF CALIFORNIA\nFOR THE COUNTY OF LOS ANGELES\nMARLAM, INC., a California Corporation;)\nKAM-RON LIMITED LIABILITY)\nCOMPANY, a Califomia Limited Liability)\nCompany; 8950 MEMORY PARK I, L.P., a)\nCalifornia Limited Partnership; KAMRAN)\nMEHDIZADEH, an individual; and MEHRAN)\nMEHDIZADEH, an individual, 3\n_ _ )\nPlaintiff; )\n_ )\n)\nvs. )\n)\n, )\nBANK OF AMERICA CORPORATION, a)\nDelaware Corporation; WACHOVIA)\nCOMMERCIAL MORTGAGE, INC., a NEW)\nJERSEY CORPORATION; WACHOVLA)\nFINANCIAL SERVICES, INC., a North)\nCarolina Corporation: CWCAPITAL ASSET)\nMANAGEMENT, a Delaware Limited)\nLiability Company; CWCAPITAL)\nFINANCLAL SERVICES, LLC, a Delaware)\nLimited Liability Company; WELLS FARGO)\nAND COMPANY, a Califomia Corporation; )\nASSURED LENDER SERVICES, INC., a)\nCalifomia; LAWYERS TITLE COMPANY,)\nINC., a Califomia Corporation; and DOES l)\nCASE NO. PC048384\nThe Honorable Randy Rhodes\nPLAINTIFFS’ NOTICE OF MOTION AND\nMOTION FOR PRELIMINARY\nINJUNCTION; MEMORANDUM OF\nPOINTS AND AUTHORITIES;\nDECLARATION OF KAMRAN\nIVIEHDIZADEH; DECLARATION OF ALAN\nLEVY;[PROPOSED] ORDER\nDate: July 1, 2010 Y\nTime: 8: 30 a.m.\nDept.: 50\n5'\n0%\nrg .’3‘\n§ as\n2 9\nE\n'11 CJ\n53 Sb\n2 E\nO\n: 5.\nO S”\nLf:\nC\n'=1\n \nO\n'-i\nP1\nC\n \nG \\/\\/§/\\/\\/\n \nO\nQ\nO\n \nE\nF .\nPl(\n \nll!\nE\n><\nF1\nE\n \n»-1\nPI(\nO\nZ\nZ\n£11\n \nE\n \nO\n'11\n'U\n§2\nga\n \nO\n \n'rim\n#O\nE\nQ:\n \n*ggi-\n°E\n3-1\nE5\nU2\n \n~§\nE\nZ\nFJ\nI\n \nE\na\n \nAction Filed: June 1, 2010\nTrial Date: None Set\n \n","title":"2010.06.03_-_P_-_Mariam_s_Notice_of_Motion_and_Motion_for_Preliminary_Injunction__Declaration_with_exhibits.txt","id":"7"},{"snippet":"Case 13-12211-BLS Doc 134 Filed 09/06/13 Page 1 of 4\nUNITED STATES BANKRUPTCY COURT\nDISTRICT OF DELAWARE\nIn re: Chapter 11\nCase No. 13-12211 (BLS)\nDebtors.\n>\n>\n>\nLoNGvrEW POWER, LLC, Q gf )\n>\n) (Jointly Administered)\n>\nNOTICE OF COMMENCEMENT OF CHAPTER 11 CASES,\nMEETING OF CREDITORS AND FIXING OF CERTAIN DATES\nOn August 30, 2013, the above-captioned debtors and debtors in possession in the\nabove-captioned cases (collectively, the “Debtors”) filed voluntary petitions for relief under\nchapter 11 of title 11 of the United States Code (the “Bankruptcy Code”) in the United States\nBankruptcy Court of the District of Delaware (the “Court”). The Debtors, and their respective\naddresses, case numbers and federal tax identification numbers, are as follows:\nDEBTOR ADDRESS CASE NO. EID#\n(Other names, if any, used by\nthe Debtor in the last 6 years\nappear in brackets)\nMorgantown, WV 26501\nMorgantown, WV 26501\nMorgantown, WV 26501\nPennsylvania, LLC Morgantown, WV 26501\nMorgantown, WV 26501\nHoldings C, LLC Maidsville, WV 26541\n1 The Debtors in these chapter 11 cases and the last four digits of each Debtor’s taxpayer identification are as\nfollows: (a) Longview Power, LLC (1860); and Longview Intermediate Holdings C, LLC (1008) (collectively,\nthe “Longview Debtors”); and (b) Mepco Holdings, LLC (6654); Mepco Intermediate Holdings A, LLC (0502);\nMepco Intermediate Holdings, LLC (4248); Mepco, LLC (3172); Coresco, LLC (6397); Dana Mining Company\nof Pennsylvania, LLC (8721); Dana Mining Company, LLC (4499); Mepco Conveyor, LLC (0477); Shannopin\nMaterials LLC (1616); Border Energy, LLC (2798); and Alternate Energy, LLC (2428) (the foregoing\nexcluding the Longview Debtors, collectively, the “Mepco Debtors”). The Longview Debtors’ principal offices\nare located at 966 Crafts Run Road, Maidsville, West Virginia 26541. The Mepco Debtors’ principal offices\nare located at 308 Dents Run Road, Morgantown, West Virginia 26501.\nRLFl 93262l7v.l\n \n","title":"ALongCourtLink_Dockets_US-BAN-DEB_1_13bk12211_Idx_1964795_9.22.2013_170558031_-_Copy_-_Copy.txt","id":"8"},{"snippet":" \n","title":"Document-1.txt","id":"9"},{"snippet":"Attendance Report\nAttendance Duration : 08/12/2014 – 28/01/2015\n\tIntern Name\n\tNo. of workings days\n\tNo. of days present\n\tPresent %\n \n\tKovalan. R\n \n\t98.3\n \n\tMadhan Ram .R\n \n\t98.3\n \n\tKeerthana Barani .S\n \n\t98.3\n  \n                                                                                                            Signature Of Manager\n","title":"AttendanceReport.txt","id":"10"}],"MultilingualClustering.majorityLanguage":"","processing-time-total":1443,"clusters":[{"score":1.0,"phrases":["Big Data"],"documents":["0","3","5"],"attributes":{"label-scores":[1.0],"score":1.0},"id":0,"size":3},{"score":0.9886590715653816,"phrases":["Product"],"documents":["1","4","5"],"attributes":{"label-scores":[0.9774467597885224],"score":0.9886590715653816},"id":1,"size":3},{"score":0.9038857653933234,"phrases":["LLC"],"documents":["7","8"],"attributes":{"label-scores":[0.9485186290792322],"score":0.9038857653933234},"id":2,"size":2},{"score":0.8813790081107877,"phrases":["Manager"],"documents":["5","10"],"attributes":{"label-scores":[0.9018704888576431],"score":0.8813790081107877},"id":3,"size":2},{"score":0.8637180923301334,"phrases":["West"],"documents":["6","8"],"attributes":{"label-scores":[0.86608956191583],"score":0.8637180923301334},"id":4,"size":2},{"score":0.8565374002820183,"phrases":["Park"],"documents":["6","7"],"attributes":{"label-scores":[0.8517486084729663],"score":0.8565374002820183},"id":5,"size":2},{"score":0.8565374002820183,"phrases":["Report"],"documents":["4","10"],"attributes":{"label-scores":[0.8517486084729663],"score":0.8565374002820183},"id":6,"size":2},{"score":0.8511422064480444,"phrases":["Arun"],"documents":["1","6"],"attributes":{"label-scores":[0.841052341968646],"score":0.8511422064480444},"id":7,"size":2},{"score":0.8468605590794246,"phrases":["Chennai"],"documents":["3","6"],"attributes":{"label-scores":[0.8326118441790135],"score":0.8468605590794246},"id":8,"size":2},{"score":0.8323898635601802,"phrases":["Letter"],"documents":["0","6"],"attributes":{"label-scores":[0.8044005088845125],"score":0.8323898635601802},"id":9,"size":2},{"score":0.0,"phrases":["Other Topics"],"documents":["2","9"],"attributes":{"label-scores":[-1.0],"score":0.0,"other-topics":true},"id":10,"size":2}]});